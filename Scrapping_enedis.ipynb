{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "url_visited = []\n",
    "url_base = 'https://www.enedis.fr/documents'\n",
    "url = url_base\n",
    "\n",
    "for num_page in range(500):\n",
    "    if num_page % 10 == 0:\n",
    "        print(num_page)\n",
    "    response = requests.get(url)\n",
    "    url_visited.append(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    page_content = soup.select(\".press-page__field\")\n",
    "    scrapping = []\n",
    "    scrap_elem = []\n",
    "    sep_list = [elem == page_content[0] for elem in page_content]\n",
    "\n",
    "    for i in range(1, len(sep_list)):\n",
    "        if sep_list[i]:\n",
    "            scrapping.append(scrap_elem)\n",
    "            scrap_elem = []\n",
    "        else:\n",
    "            scrap_elem.append([elem for elem in page_content[i].contents if elem != '\\n'])\n",
    "    scrapping.append(scrap_elem)\n",
    "\n",
    "    for pdf in scrapping:\n",
    "        new_data = [[\"\"]] * 5\n",
    "        for elem in pdf:\n",
    "            if type(elem) == list and len(elem):\n",
    "                if \"attrs\" in elem[0].__dict__ and 'href' in elem[0].__dict__.get(\"attrs\") and 'aria-label' not in elem[0].__dict__.get(\"attrs\"):\n",
    "                    new_data[0] = \"https://www.enedis.fr\" + elem[0].__dict__.get(\"attrs\").get('href')\n",
    "                elif \"attrs\" in elem[0].__dict__ and 'datetime' in elem[0].__dict__.get(\"attrs\"):\n",
    "                    new_data[2] = elem[0].__dict__.get(\"attrs\").get('datetime')\n",
    "                elif \"attrs\" in elem[0].__dict__ and 'aria-label' in elem[0].__dict__.get(\"attrs\"):\n",
    "                    new_data[4] = elem[0].__dict__.get(\"attrs\").get('aria-label')\n",
    "                elif new_data[1] == [\"\"]:\n",
    "                    new_data[1] = str(elem[0]).strip().replace(\",\", \";\")\n",
    "                else:\n",
    "                    new_data[3] = str(elem[0]).strip().replace(\",\", \";\")\n",
    "        data.append(new_data)\n",
    "\n",
    "    next = soup.select(\".pager__next\")\n",
    "    if not next:\n",
    "        break\n",
    "    url = url_base + next[0].attrs.get(\"href\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pdf_enedis = pd.DataFrame(data, columns=['url', 'type', \"date\", \"content\", \"file_info\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pdf_enedis.to_csv(\"documents_enedis.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
